{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "import xgboost\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics given the actual and predicted labels.\n",
    "    Returns the macro-F1 score, the accuracy, the flip error rate and the\n",
    "    mean absolute error (MAE).\n",
    "    The flip error rate is the percentage where an instance was predicted \n",
    "    as the opposite label (i.e., left-vs-right or high-vs-low).\n",
    "    \"\"\"\n",
    "    # calculate macro-f1\n",
    "    f1 = f1_score(actual, predicted, average='macro') * 100\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(actual, predicted) * 100\n",
    "    \n",
    "    # calculate the flip error rate\n",
    "    flip_err = sum([1 for i in range(len(actual)) if abs(actual[i] - predicted[i]) > 1]) / len(actual) * 100\n",
    "    \n",
    "    # calculate mean absolute error (mae)\n",
    "    mae = sum([abs(actual[i] - predicted[i]) for i in range(len(actual))]) / len(actual)\n",
    "    mae = mae[0] if not isinstance(mae, float) else mae\n",
    "    \n",
    "    return f1, accuracy, flip_err, mae\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_xgboost_gridsearch(X, y, featurename, prefix):\n",
    "\n",
    "    X_train, X_test, y_train_str, y_test_str = train_test_split(X, y, test_size=0.20, random_state=999)\n",
    "\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train_str)\n",
    "    y_test = encoder.fit_transform(y_test_str)\n",
    "\n",
    "    # normalize the features values\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    xgb_param_grid = {\"max_depth\"        : [ 4, 5, 6, 8, 10, 12],\n",
    "                      \"min_child_weight\" : [ 3, 5, 7, 9, 11, 13 ] }    \n",
    "\n",
    "    clf_cv = GridSearchCV(estimator = XGBClassifier( use_label_encoder=False, learning_rate =0.1, n_estimators=140, \n",
    "                                                      max_depth=5, min_child_weight=1, gamma=0.1, \n",
    "                                                      subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', nthread=4, \n",
    "                                                      scale_pos_weight=1, seed=27), \n",
    "                              param_grid = xgb_param_grid, scoring='roc_auc',\n",
    "                              n_jobs=8, cv=5,\n",
    "                             return_train_score=True)\n",
    "    \n",
    "    \n",
    "    clf_cv.fit(X_train, y_train)\n",
    "    \n",
    "    print(clf_cv.best_params_, clf_cv.best_score_)\n",
    "    \n",
    "    clf = xgboost.XGBClassifier(use_label_encoder=False, learning_rate =0.1, n_estimators=140, \n",
    "                                max_depth=clf_cv.best_params_[\"max_depth\"],\n",
    "                                min_child_weight=clf_cv.best_params_[\"min_child_weight\"],\n",
    "                                gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "                                  objective= 'binary:logistic', nthread=4, \n",
    "                                  scale_pos_weight=1, seed=27)    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    file_name = f\"./models/{prefix}_{featurename}.plk\"\n",
    "    pickle.dump(clf, open(file_name, \"wb\"))\n",
    "    \n",
    "    # generate predictions\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    # calculate the performance metrics on the whole set of predictions (5 folds all together)\n",
    "    actual = y_test\n",
    "    predicted = pred\n",
    "    results = calculate_metrics(actual, predicted)\n",
    "\n",
    "    return clf, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get Profile Description encoded by Setence BERT (SBERT) feature names\n",
    "feat_description_sbert = []\n",
    "for cat in range(768):\n",
    "    feat_description_sbert.append(\"desc\"+str(cat))\n",
    "\n",
    "### get User Tweets encoded by SBERT feature names\n",
    "feat_precovid_tweets_sbert = []\n",
    "for cat in range(768):\n",
    "    feat_precovid_tweets_sbert.append(\"s\"+str(cat))    \n",
    "    \n",
    "### get NELA feature names \n",
    "feat_nela = [\"quotes\", \"exclaim\", \"allpunc\", \"allcaps\", \"stops\", \"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\", \"UH\", \"WP$\", \"WRB\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"$\", \"\\'\\'\", \"(\", \")\", \",\", \"--\", \".\", \":\", \"``\", \"ttr\", \"avg_wordlen\", \"word_count\", \"flesch_kincaid_grade_level\", \"smog_index\", \"coleman_liau_index\", \"lix\", \"bias_words\", \"assertatives\", \"factives\", \"hedges\", \"implicatives\", \"report_verbs\", \"positive_opinion_words\", \"negative_opinion_words\", \"vadneg\", \"vadneu\", \"vadpos\", \"wneg\", \"wpos\", \"wneu\", \"sneg\", \"spos\", \"sneu\", \"HarmVirtue\", \"HarmVice\", \"FairnessVirtue\", \"FairnessVice\", \"IngroupVirtue\", \"IngroupVice\", \"AuthorityVirtue\", \"AuthorityVice\", \"PurityVirtue\", \"PurityVice\", \"MoralityGeneral\"]\n",
    "\n",
    "### get Following top users  feature names\n",
    "feat_following_top = []\n",
    "for cat in range(95):\n",
    "    feat_following_top.append(\"followtop_\"+str(cat))\n",
    "    \n",
    "### get LIWC feature names \n",
    "feat_liwc = ['function (Function Words)', 'pronoun (Pronouns)', 'ppron (Personal Pronouns)', 'i (I)', 'we (We)', 'you (You)', 'shehe (SheHe)', 'they (They)', 'ipron (Impersonal Pronouns)', 'article (Articles)', 'prep (Prepositions)', 'auxverb (Auxiliary Verbs)', 'adverb (Adverbs)', 'conj (Conjunctions)', 'negate (Negations)', 'verb (Verbs)', 'adj (Adjectives)', 'compare (Comparisons)', 'interrog (Interrogatives)', 'number (Numbers)', 'quant (Quantifiers)', 'affect (Affect)', 'posemo (Positive Emotions)', 'negemo (Negative Emotions)', 'anx (Anx)', 'anger (Anger)', 'sad (Sad)', 'social (Social)', 'family (Family)', 'friend (Friends)', 'female (Female)', 'male (Male)', 'cogproc (Cognitive Processes)', 'insight (Insight)', 'cause (Causal)', 'discrep (Discrepancies)', 'tentat (Tentative)', 'certain (Certainty)', 'differ (Differentiation)', 'percept (Perceptual Processes)', 'see (See)', 'hear (Hear)', 'feel (Feel)', 'bio (Biological Processes)', 'body (Body)', 'health (Health)', 'sexual (Sexual)', 'ingest (Ingest)', 'drives (Drives)', 'affiliation (Affiliation)', 'achieve (Achievement)', 'power (Power)', 'reward (Reward)', 'risk (Risk)', 'focuspast (Past Focus)', 'focuspresent (Present Focus)', 'focusfuture (Future Focus)', 'relativ (Relativity)', 'motion (Motion)', 'space (Space)', 'time (Time)', 'work (Work)', 'leisure (Leisure)', 'home (Home)', 'money (Money)', 'relig (Religion)', 'death (Death)', 'informal (Informal Language)', 'swear (Swear)', 'netspeak (Netspeak)', 'assent (Assent)', 'nonflu (Nonfluencies)', 'filler (Filler Words)']\n",
    "\n",
    "features = {}\n",
    "features['stat'] = ['days_since_join', 'user_favourites_count', 'user_followers_count', 'user_friends_count', 'user_statuses_count', 'user_verified']\n",
    "features['description'] = feat_description_sbert\n",
    "features['following'] = feat_following_top\n",
    "\n",
    "features['stat_description'] = features['stat'] + features['description']\n",
    "features['description_following'] = features['description'] + features['following']\n",
    "features['stat_following'] = features['stat'] + features['following']\n",
    "features['stat_description_following'] = features['stat'] + features['description'] + features['following']\n",
    "\n",
    "features['media'] = ['extremeleft','left','leftcenter','center','rightcenter','right','extremeright','very high','high','mostly factual','mixed','low','very low','questionable source']\n",
    "features['NELA'] = feat_nela\n",
    "features['LIWC'] = feat_liwc\n",
    "features['tweets'] = feat_precovid_tweets_sbert\n",
    "\n",
    "features['media_NELA'] = features['media']+features['NELA']\n",
    "features['media_LIWC'] = features['media']+features['LIWC']\n",
    "features['media_tweets'] = features['media']+features['tweets']\n",
    "features['NELA_LIWC'] = features['NELA']+features['LIWC']\n",
    "features['NELA_tweets'] = features['NELA']+features['tweets']\n",
    "features['LIWC_tweets'] = features['LIWC']+features['tweets']\n",
    "\n",
    "features['media_NELA_LIWC'] = features['media']+features['NELA']+features['LIWC']\n",
    "features['NELA_LIWC_tweets'] = features['NELA']+features['LIWC']+features['tweets']\n",
    "features['media_LIWC_tweets'] = features['media']+features['LIWC']+features['tweets']\n",
    "features['media_NELA_tweets'] = features['media']+features['NELA']+features['tweets']\n",
    "features['media_NELA_LIWC_tweets'] = features['media']+features['NELA']+features['LIWC']+features['tweets']\n",
    "\n",
    "features['ALL'] = features['stat_description_following'] + features['media_NELA_LIWC_tweets']\n",
    "\n",
    "## BEST combination for Hateful User Prediction\n",
    "features['stat_description_following_media_LIWC'] = features['stat_description_following'] + features['media_LIWC']\n",
    "features['stat_description_following_media_tweets'] = features['stat_description_following'] + features['media_tweets']\n",
    "\n",
    "## BEST combination for High-level Hateful User Prediction\n",
    "features['stat_following_media_NELA_LIWC'] = features['stat_following'] + features['media_NELA_LIWC']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset (features) \n",
    "\n",
    "A column named 'y' is the lable for whether a user belongs to hateful or reference user group (Hateful user prediction (Task 1)). \n",
    "Y=1 is hateful and Y=0 is reference. \n",
    "\n",
    "A column named 'y_level' is the labe for high- and low-level hateful users .\n",
    "Y=1 is high-level hateful user, Y=0 is low-level hateful users, Y=-1 is reference use and excluded from High-level hateful user prediction (Task1). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that \"user_features_hate_ref_SAMPLE.tsv\" is a sample file with 80 rows, just for reference.\n",
    "df = pd.read_csv(\"user_features_hate_ref_SAMPLE.tsv\", sep=\"\\t\")\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Hateful Users (Task 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"./pred_results_xgboost.tsv\", \"w\") as output:\n",
    "\n",
    "    output.write(\"\\t\".join([\"Feature Set\", \"Macro-F1\", \"Accuracy\", \"Flip error-rate\", \"MAE\"])+\"\\n\")\n",
    "\n",
    "    for featurename in features.keys():\n",
    "        print(featurename, \"------\")\n",
    "        featurelist = features[featurename]\n",
    "\n",
    "        X = df[featurelist]\n",
    "        print(X.shape)\n",
    "        y = df['y']\n",
    "\n",
    "        best_model, results= train_and_test_xgboost_gridsearch(X, y, featurename, \"xgb\")\n",
    "\n",
    "        results_str = \"\\t\".join([str(each_result) for each_result in results])\n",
    "        output.write(\"\\t\".join([featurename, results_str])+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting High-level Hateful Users (Task 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate = df.query(\"y_level != -1\")\n",
    "print(df_hate.y_level.value_counts())\n",
    "\n",
    "with open(f\"./pred_results_xgboost_level.tsv\", \"w\") as output:\n",
    "\n",
    "    output.write(\"\\t\".join([\"Feature Set\", \"Macro-F1\", \"Accuracy\", \"Flip error-rate\", \"MAE\"])+\"\\n\")\n",
    "\n",
    "    for featurename in features.keys():\n",
    "        print(featurename)\n",
    "        featurelist = features[featurename]\n",
    "\n",
    "        X = df_hate[featurelist]\n",
    "        print(X.shape)\n",
    "        y = df_hate['y_level']\n",
    "\n",
    "        best_model, results = train_and_test_xgboost_gridsearch(X, y, featurename, \"level_xgb\")\n",
    "\n",
    "        results_str = \"\\t\".join([str(each_result) for each_result in results])\n",
    "        output.write(\"\\t\".join([featurename, results_str])+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
