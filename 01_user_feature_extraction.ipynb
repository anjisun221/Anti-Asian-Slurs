{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features \n",
    "\n",
    "## Content-agnostic features\n",
    "* User statistics: Days since join, favourites count, follower count, friends counts, statuses count, user verified\n",
    "* User profile description: SBERT encoding\n",
    "* User following: Select top 50 accounts from each group, and get the union set of users to create a feature vector (number of dimensions is 95 in our case) \n",
    "\n",
    "\n",
    "## Content features\n",
    "* Media: Using the URLs in users' tweets, create a vector  (the number of dimentions is 15 : 'extremeleft','left','leftcenter','center','rightcenter','right','extremeright','very high','high','mostly factual','mixed','low','very low','questionable source')\n",
    "* NELA: https://github.com/BenjaminDHorne/Language-Features-for-News\n",
    "* LIWC: No code for LIWC as I used the LIWC software. \n",
    "* Tweets: SBERT encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT encoding \n",
    "\n",
    "For each user, we randomly sampled N tweets, where N is the median number of tweets per users. \n",
    "\n",
    "The input file needs to have at lest two fields: 'userid' and 'text' (tweet). \n",
    "\n",
    "We encode each tweet with SBERT and get the mean values to represent each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"userid\"]\n",
    "for cat in range(768):\n",
    "    headers.append(\"s\"+str(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that you need your own input file, which needs at least two fields: tweetid and text. For each user, we have a sample set of tweets.\n",
    "inputfilename = \"YOUR_SAMPLED_TWEETS.tsv\"\n",
    "out_dir = \"user_features/sbert_precovid\"\n",
    "outfilename = f'{out_dir}/userid2sbert_emd_YOUR_SAMPLED_TWEETS.tsv'\n",
    "\n",
    "df = pd.read_table(inputfilename, sep=\"\\t\")\n",
    "print(df.shape)\n",
    "user_ids = df['user_id'].unique()\n",
    "print(len(user_ids))\n",
    "\n",
    "with open(outfilename, \"w\") as output:\n",
    "    output.write(\"\\t\".join(headers)+\"\\n\")\n",
    "\n",
    "    for cnt, each_id in enumerate(user_ids):\n",
    "        df_tmp = df.query(\"user_id == @each_id\")\n",
    "\n",
    "        sentence_embeddings = model.encode(list(df_tmp['text']))\n",
    "        mean_embedding = sentence_embeddings.mean(axis=0)\n",
    "\n",
    "        emb_feat = \"\\t\".join([str(each) for each in mean_embedding])\n",
    "\n",
    "        output.write(f'{each_id}\\t{emb_feat}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NELA faeture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nela_features.nela_features import NELAFeatureExtractor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nela = NELAFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To print out the header for the output feature file \n",
    "newsarticle = \"Breaking News: Ireland Expected To Become World's First Country To Divest From Fossil Fuels ...\" \n",
    "all_feature_names = []\n",
    "feature_vector, feature_names = nela.extract_style(newsarticle) \n",
    "all_feature_names += feature_names\n",
    "feature_vector, feature_names = nela.extract_complexity(newsarticle) \n",
    "all_feature_names += feature_names\n",
    "feature_vector, feature_names = nela.extract_bias(newsarticle)\n",
    "all_feature_names += feature_names\n",
    "feature_vector, feature_names = nela.extract_affect(newsarticle) \n",
    "all_feature_names += feature_names\n",
    "feature_vector, feature_names = nela.extract_moral(newsarticle)\n",
    "all_feature_names += feature_names\n",
    "# all_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nela(text):\n",
    "    myvec = []\n",
    "    feature_vector, feature_names = nela.extract_style(text) \n",
    "    myvec += feature_vector\n",
    "    feature_vector, feature_names = nela.extract_complexity(text) \n",
    "    myvec += feature_vector\n",
    "    feature_vector, feature_names = nela.extract_bias(text)\n",
    "    myvec += feature_vector\n",
    "    feature_vector, feature_names = nela.extract_affect(text) \n",
    "    myvec += feature_vector\n",
    "    feature_vector, feature_names = nela.extract_moral(text)\n",
    "    myvec += feature_vector\n",
    "    \n",
    "    return myvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that you need your own input file, which needs at least two fields: tweetid and text. For each user, we have a sample set of tweets.\n",
    "inputfilename = \"YOUR_SAMPLED_TWEETS.tsv\"\n",
    "\n",
    "df = pd.read_table(inputfilename, sep=\"\\t\")\n",
    "print(df.shape)\n",
    "user_ids = df['user_id'].unique()\n",
    "print(len(user_ids))\n",
    "\n",
    "cnt_error = 0\n",
    "outfilename = \"userid2language_nela_features_YOUR_SAMPLED_TWEETS.tsv\"\n",
    "with open(outfilename, \"w\") as output:\n",
    "    output.write(\"user_id\\t%s\\n\" % (\"\\t\".join(all_feature_names)))\n",
    "\n",
    "    for each_id in user_ids:\n",
    "        df_tmp = df.query(\"user_id == @each_id\")\n",
    "\n",
    "        list_vecs = []\n",
    "        for text in list(df_tmp['text']):\n",
    "            try:\n",
    "                myvec = compute_nela(text)\n",
    "                list_vecs.append(np.array(myvec))\n",
    "            except:\n",
    "                cnt_error +=1\n",
    "                pass\n",
    "            \n",
    "        myvec_mean = np.array(list_vecs).mean(axis=0)\n",
    "\n",
    "        mystr = \"\\t\".join([str(each) for each in myvec_mean])\n",
    "        output.write(\"%s\\t%s\\n\" % (each_id, mystr))\n",
    "print(\"cnt_error=\", cnt_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Media feature \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tld import get_tld\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading data of media bias and facuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbfc = pd.read_csv(\"./mbfc_final_jisun_20200719.tsv\", sep=\"\\t\")\n",
    "print(df_mbfc.shape)\n",
    "df_mbfc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain2bias = {}\n",
    "domain2factual = {}\n",
    "\n",
    "for index, row in df_mbfc.iterrows():\n",
    "    try:\n",
    "        domain = get_tld(row['source_url'], as_object=True).fld\n",
    "        bias = row['bias']\n",
    "        factual = row['factual']\n",
    "\n",
    "        domain2bias[domain] = bias\n",
    "        domain2factual[domain] = factual  \n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map URLs to bias and facuality\n",
    "1. Input file: expanded_urls_YOUR_FILE.tsv - an input file with the following fields: user_screen_name, tweet_id, url, expanded_url\n",
    "2. Output file: expanded_urls_domain_bias_factual_YOUR_FILE.tsv - user_screen_name, tweet_id, domain, bias, factual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"expanded_urls_YOUR_FILE.tsv\") as fi, open(\"expanded_urls_domain_bias_factual_YOUR_FILE.tsv\", \"w\") as output:\n",
    "    output.write(\"\\t\".join(['user_screen_name', 'tweet_id', 'domain', 'bias', 'factual'])+\"\\n\")\n",
    "\n",
    "    for line_cnt, line in enumerate(fi):\n",
    "        \n",
    "        user_screen_name, tweet_id, url, expanded_url = [term.strip() for term in line.split(\"\\t\")]\n",
    "        \n",
    "        try:\n",
    "            if 'twitter.com' in expanded_url: ## it's easier to skip Twitter urls. \n",
    "                continue\n",
    "                \n",
    "            domain = get_tld(expanded_url, as_object=True).fld\n",
    "            \n",
    "            if domain in domain2bias:\n",
    "                bias = domain2bias[domain]\n",
    "                factual = domain2factual[domain]\n",
    "                output.write(\"\\t\".join([user_screen_name, tweet_id, domain, bias, factual])+\"\\n\")\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a media feature vector per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_url = pd.read_csv(\"expanded_urls_domain_bias_factual_YOUR_FILE.tsv\", sep=\"\\t\")\n",
    "print(\"df_url=\", df_url.shape)    \n",
    "user_ids = df_url['user_id'].unique()\n",
    "print(len(user_ids))\n",
    "\n",
    "with open(\"urls_by_bias_precovid_YOUR_FILE.tsv\", \"w\") as output:\n",
    "    output.write(\"\\t\".join([\"user_id\", \"extremeleft\", \"left\", \"leftcenter\", \"center\", \"rightcenter\", \"right\", \"extremeright\", \"very high\", \"high\", \"mostly factual\", \"mixed\", \"low\", \"very low\", \"questionable source\"])+\"\\n\")\n",
    "\n",
    "    for each_id in user_ids:\n",
    "        df_url_tmp = df_url.query(\"user_id == @each_id\")\n",
    "        dict_bias = df_url_tmp['bias'].value_counts().to_dict()\n",
    "        dict_factual = df_url_tmp['factual'].value_counts().to_dict()\n",
    "\n",
    "        result_bias = []\n",
    "        for each in [\"extremeleft\", \"left\", \"leftcenter\", \"center\", \"rightcenter\", \"right\", \"extremeright\"]:\n",
    "            if each in dict_bias:\n",
    "                result_bias.append(str(dict_bias[each]))\n",
    "            else:\n",
    "                result_bias.append(\"0\")\n",
    "\n",
    "        result_factual = []\n",
    "\n",
    "        for each in [\"very high\", \"high\", \"mostly factual\", \"mixed\", \"low\", \"very low\", \"questionable source\"]:\n",
    "            if each in dict_factual:\n",
    "                result_factual.append(str(dict_factual[each]))\n",
    "            else:\n",
    "                result_factual.append(\"0\")\n",
    "\n",
    "        myresult = [str(each_id)]\n",
    "        myresult += result_bias\n",
    "        myresult += result_factual\n",
    "\n",
    "        output.write(\"\\t\".join(myresult)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
